# -*- coding: utf-8 -*-
"""Abhi-Adduri-SC-Interview.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1plFU1AZbu03hiHFygj-BaMUFyoUaMds6

# Abhi Adduri - SC-Interview Task Data Analysis

The following notebook contains code that you can run to reproduce any of the results shown. For the most part, I will use this notebook as a markdown document to analyze the data and show plots easily. My repository also contains standalone scripts with usage guides, which is how I generated my results.

The final comparison across methods as reported below:

| | | | | | | |
|-|-|-|-|-|-|-|
|Method|Features|Micro AUROC|Micro AUPR|Macro AUPR|Micro F1|Macro F1|
|LR|Raw|0.892|0.684|0.603|0.643|0.571|
|RF|Raw|0.960|0.767|0.633|0.679|0.489|
|NN|Raw|0.979|0.852|0.713|0.756|0.644|
|LR|HVG|0.975|0.831|0.670|0.779|0.669|
|RF|HVG|0.971|0.810|0.669|0.722|0.571|
|NN|HVG|**0.989**|**0.920**|**0.783**|**0.837**|**0.700**|
|LR|scGPT|0.973|0.819|0.663|0.765|0.656|
|RF|scGPT|0.972|0.819|0.675|0.727|0.567|
|NN|scGPT|0.988|0.918|0.780|0.832|**0.700**|


A general overview of this notebook:

1. First we will visualize and inspect the data.
2. We will fit baseline classifiers to the data, and a simple neural network, and compute the above metrics.
3. We observe a large gap in the metrics on the training and test sets, so we try alternate featurizations, and see large improvements using the highly-variable genes (HVG), and a foundation model (scGPT).
4. Ideas for future improvements.

All results were separately validated on a cluster with ten different random seeds. The results shown here are for a single train / validation / test split that stratifies the data such that every class is present in each split.

# Setup

If you want to run these scripts on a standalone server and not a colab notebook, you may need to create a conda environment to make sure flash attention installs ok:



```
conda create -n scgpt python=3.9
conda activate scgpt
conda install pip
conda install -c nvidia cuda-toolkit
```

# Imports and load data
"""

# Commented out IPython magic to ensure Python compatibility.
# Install package requirements if you want to run in this notebook.
! pip install packaging wandb torch numpy scanpy matplotlib gdown tqdm --quiet
! pip install scgpt "flash-attn<1.0.5" --quiet # This will take a while...

# Clone my fork of the interview repository and chdir into it
! git clone https://github.com/abhinadduri/SC-interview.git
# %cd SC-interview

# Make folders to store files for metric visualization down the line
! mkdir plots
! mkdir embeddings

# Download the requisite data
! gdown 1nsmQHdWek4YzIfKs9xUnLBHxKBWu8hUJ -O cells.npy

# Needed to generate embeddings with scGPT foundation model
! gdown https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y -O scgpt --folder
! wget https://github.com/bowang-lab/scGPT/files/13243634/gene_info.csv -P scgpt

# Disable logging to W&B for this colab notebook
# %env WANDB_MODE=offline
# %env WANDB_SILENT=true

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scanpy as sc
import seaborn as sns
from IPython.display import Image

# Create an AnnData object for our data
cells = np.load('cells.npy', allow_pickle=True).item()
adata = sc.AnnData(X=cells['UMI'].toarray(), obs={'cell_type': cells['classes']}, var={'gene_id': cells['gene_ids']})

# Basic preprocessing following scanpy suggestions
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

"""# Data Inspection and Visualization

We first want to visually inspect the data to see how separable the cell type clusters are, and also how imbalanced the classes are.
"""

# Calculate the proportion of each cell type
cell_type_counts = adata.obs['cell_type'].value_counts()
cell_type_proportions = cell_type_counts / len(adata)

plt.figure(figsize=(12, 6))
sns.barplot(x=cell_type_proportions.index, y=cell_type_proportions.values)

plt.title('Cell Type Distribution')
plt.xlabel('Cell Type')
plt.ylabel('Proportion')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Add percentage labels on top of each bar
for i, v in enumerate(cell_type_proportions):
    plt.text(i, v, f'{v:.1%}', ha='center', va='bottom')

plt.show()

"""We see that the distribution of labels is very long tailed, and we can expect that classification of CD34+ and CD4+ T Helper2 might be difficult.

Let's also pre-compute several common transforms that scanpy offers to give us more insight into the data. Since most of the gene expression data seems sparse, we can start by computing the set of highly-variable genes.
"""

# Compute common transforms using scanpy
sc.pp.highly_variable_genes(adata)
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)

sc.tl.pca(adata, svd_solver='arpack')
sc.tl.umap(adata)
sc.tl.leiden(adata)

# Create confusion matrix
cluster_cell_type = pd.crosstab(adata.obs['leiden'], adata.obs['cell_type'])

with plt.rc_context({"figure.figsize": (8, 6), "figure.dpi": (300)}):
    sc.pl.pca(adata, color=['leiden', 'cell_type'], title=['Leiden Cluster Assignments', 'True Cell Types'], size=10)

"""From here we see there is quite a large overlap with CD56+ NK and CD8+ Cytotoxic T. There is also a blob of similar looking points consisting of CD8+ Cytotoxic T, CD8+/CD45RA + Naive Cytotoxic, CD19+ B, and CD4+/CD25T Reg, suggesting these classes may be difficult to discriminate."""

with plt.rc_context({"figure.figsize": (8, 6), "figure.dpi": (300)}):
    sc.pl.umap(adata, color=['leiden', 'cell_type'], title=['Leiden Cluster Assignments', 'True Cell Types'], size=10)

"""Lastly let's take a look at a confusion matrix of the clusters vs the true cell type annotations."""

confusion_matrix = pd.crosstab(adata.obs['leiden'], adata.obs['cell_type'], normalize='index')
plt.figure(figsize=(12, 10))
sns.heatmap(confusion_matrix, annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Leiden Clusters vs Known Cell Types')
plt.tight_layout()
plt.show()

"""This shows that a few cell types appear across a lot of clusters. Furthermore, cells like CD4+ T Helper2 are co-located with Dendritic cells in PCA space, and CD4+/CD45A+/CD25- Naive T are co-located with CD8+/CD45A+ naive Cytotoxic cells, which will likely make their discrimination difficult.

# Baseline Models

Let's start by computing some baselines on the raw data. This will be expensive but can guide our analysis. We will train a logistic regression model, a random forest classifier, and a simple neural network on the normalized data. For these baselines we will use the balanced class weight option to address the cell type imbalance in our dataset.
"""

! python train.py --method lr --verbose

! python train.py --method rf --verbose

"""Now that we have baselines, let's see if a neural network approach can improve our results."""

! python train.py --method nn --verbose --loss-plot plots/raw_nn_loss_50.png
Image('plots/raw_nn_loss_50.png', width=720, height=432)

"""We overfit very heavily! Let's try 10 epochs instead (early stopping)."""

! python train.py --method nn --epochs 10 --loss-plot plots/raw_nn_loss_10.png
Image('plots/raw_nn_loss_10.png', width=720, height=432)

"""Let's visualize ROC and PR curves for the three classifiers."""

# Commented out IPython magic to ensure Python compatibility.
# %run utils.py # this gives us a plot_curves helper function
methods = ['nn', 'rf', 'lr']
plot_curves(methods, 'roc', title="ROC Curve using raw features")
plot_curves(methods, 'prc', title="Precision-Recall Curve using raw features")
Image('plots/roc_comparison.png')

Image('plots/prc_comparison.png')

"""# Addressing overfitting by removing non-variable genes

Our neural network above has much higher AUPR on the training data than on the test data, meaning the generalization gap is large. Since we know that only a subset of our genes are highly variable, e.g. explain the variance in our dataset, let's try training on only those genes.
"""

! python embed.py --featurizer hvg --out-file hvg_embeddings.h5ad

"""We can re-run benchmarks and compute new ROC and PR curves, using the hvg embeddings as features."""

! python train.py --method lr --data hvg_embeddings.h5ad
! python train.py --method rf --data hvg_embeddings.h5ad

! python train.py --method nn --data hvg_embeddings.h5ad --loss-plot plots/hvg_nn_loss_50.png
Image('plots/hvg_nn_loss_50.png', width=720, height=432)

"""The validation curve is likely lower than the training curve for two reasons. First, we use dropout in evaluation but not training, which increases training loss. Second, the eval loss is computed after a whole epoch, whereas the training loss is aggregated per batch in the epoch. To verify nothing strange is going on, let's use the --recompute-train-loss flag."""

! python train.py --method nn --data hvg_embeddings.h5ad --loss-plot plots/hvg_nn_loss_50_corrected.png --recompute-train-loss
Image('plots/hvg_nn_loss_50_corrected.png', width=720, height=432)

"""Things make sense again now ðŸ™‚ . Let's recompute ROC and PR curves."""

# Commented out IPython magic to ensure Python compatibility.
# %run utils.py # this gives us a plot_curves helper function
methods = ['nn', 'rf', 'lr']
plot_curves(methods, 'roc', title="ROC Curve using HVG features")
plot_curves(methods, 'prc', title="Precision-Recall Curve using HVG features")
Image('plots/roc_comparison.png')

Image('plots/prc_comparison.png')

"""All methods are greatly improved by considering only the highly variable genes! Removing the sparse genes likely helps the learning algorithms focus on what's important.

# Using scGPT foundation model embeddings

Lastly, let's try using a foundation model developed for single-cell data. Here we use scGPT to compute gene embeddings and train on those.
"""

! python embed.py --featurizer scgpt --out-file scgpt_embeddings.h5ad

! python train.py --method lr --data scgpt_embeddings.h5ad
! python train.py --method rf --data scgpt_embeddings.h5ad

"""This time we can visualize the embeddings from our trained model."""

! python train.py --method nn --data scgpt_embeddings.h5ad --loss-plot scgpt_nn_loss.png --epochs 100 --output-latents embeddings/scgpt_nn.npz --recompute-train-loss --verbose
Image('scgpt_nn_loss.png', width=720, height=432)

# Commented out IPython magic to ensure Python compatibility.
# %run utils.py # this gives us a plot_curves helper function
methods = ['nn', 'rf', 'lr']
plot_curves(methods, 'roc', title="ROC Curve using scGPT Features")
plot_curves(methods, 'prc', title="Precision-Recall Curve using scGPT Features")
Image('plots/roc_comparison.png')

Image('plots/prc_comparison.png')

"""Interestingly, the foundation model performs **worse** than simply using the highly variable genes! This is worth further analysis.

# Visualizing the embedding space
"""

from sklearn.decomposition import PCA

e = np.load('embeddings/scgpt_nn.npz', allow_pickle=True)

# Compute the top two principal components
pca = PCA(n_components=2)
pca_result = pca.fit_transform(e['latents'])
cell_names_cat = pd.Categorical(e['cell_names'])
codes = cell_names_cat.codes

# Create a scatter plot
plt.figure(figsize=(12, 8))
scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=codes, cmap='viridis', alpha=0.6)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA of SCGPT embeddings colored by Cell Type')

# Add a color bar and cell types
cbar = plt.colorbar(scatter)
cbar.set_label('Cell Type')
cbar.set_ticks(range(len(cell_names_cat.categories)))
cbar.set_ticklabels(cell_names_cat.categories)

# Add legend
handles, labels = scatter.legend_elements(prop="colors", alpha=0.6)
legend = plt.legend(handles, cell_names_cat.categories, loc="upper right", title="Cell Types")

# Save the plot
plt.show()

"""# Takeaways and future directions to improve

1. It's interesting that the embeddings from the foundation model perform worse than simply filtering for the highly variable genes. This is something worth looking into further. It's possible that there is a pre-train vs fine-tune mismatch here, as I used the **whole-human (recommended)** model that was pre-trained on 33 million normal human cells. It's possible that the **continual pretrained** model, described by the authors to be used "for zero-shot cell embedding related tasks", would be a better fit.

2. The visualized embedding space shows that there is still quite some overlap between different cell types. Introducing a supervised contrastive loss on the embedding space may improve generalization.

3. Simple methods applied correctly can often lead to good results. In this case, training a simple neural network with appropriate dropout and batch normalization resulted in the best performing method.

4. Though I didn't explore this much, we could consider class weighing or resampling techniques to improve performance on the minority classes. We overfit on **CD4+ T Helper2**, which is the least represented class, but perform well on **CD34+**, the second least represented class, suggesting that CD34+ is easier to discriminate. We also perform poorly on **CD4+/CD45RA+/CD25- Naive T** and **CD4+/CD45RO+ Memory**, which also makes sense looking at the confusion matrix below.
"""

confusion_matrix = pd.crosstab(adata.obs['leiden'], adata.obs['cell_type'], normalize='index')
plt.figure(figsize=(12, 10))
sns.heatmap(confusion_matrix, annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Leiden Clusters vs Known Cell Types')
plt.tight_layout()
plt.show()

